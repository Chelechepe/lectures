{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b2dc82",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Intro Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c9ae5",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/kZJcpM3gYdlK4D64Ft/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18747c7e",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## When do we use Unsupervised Learning?\n",
    "\n",
    "**Examples:** (When there are no labels)\n",
    "\n",
    "+ Customer segmentation\n",
    "+ Product segmentation\n",
    "+ One-person segment (what works for one group doesn't necessarily work for another)\n",
    "+ Grouping according to characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab6dca",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Situation**\n",
    "\n",
    "Each element (user, customer, product...) can be considered a vector. Given a package of vectors, they are distributed by clusters. Similar vectors will be in the same cluster, and similar clusters will be 'closer' to each other than those that are more different.\n",
    "\n",
    "All this proceeding assumes the existence of a distance metric, so that distance between vectors within the vector space can be measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58af425",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Steps to apply USL:**\n",
    "\n",
    "+ **1**: Vector representation\n",
    "+ **2**: Metric, distance function\n",
    "+ **3**: Similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8992692",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "When creating a cluster, the vector in the center of the cluster (centroid) is considered the prototype or the most representative of the cluster.\n",
    "\n",
    "\n",
    "**Problems treatable by USL:**\n",
    "\n",
    "+ Market segmentation\n",
    "+ Detection of fraud in insurance or banking\n",
    "+ Response to medical treatments\n",
    "+ Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72cabbe",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Examples of models:**\n",
    "\n",
    "+ K-Means\n",
    "+ Hierarchical Clustering\n",
    "+ DBSCAN\n",
    "+ HDBSCAN\n",
    "+ GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Viz mantra\n",
    "\n",
    "%matplotlib inline\n",
    "%config Inlinebackend.figure_format= 'retina'\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={\"figure.figsize\": (12.,6.)})\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a55a7d",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## PCA\n",
    "**Main Component Analysis**\n",
    "Principal Component Analysis (PCA) is a statistical method that allows simplifying the complexity of sample spaces with many dimensions while preserving their information. Suppose there is a sample with n individuals each with p variables (X1, X2, …, Xp), that is, the sample space has p dimensions. PCA allows to find a number of underlying factors (z<p) that explain approximately the same as the original p variables. Where before p values ​​were needed to characterize each individual, now z values ​​suffice. Each of these z new variables is called the principal component.\n",
    "\n",
    "Principal Component Analysis belongs to the family of techniques known as unsupervised learning. The supervised learning methods described in previous chapters have the objective of predicting a response variable Y from a series of predictors. To do this, we have p characteristics (X1, X2 … Xp) and the response variable Y measured in n observations. In the case of unsupervised learning, the response variable Y is not taken into account since the objective is not to predict Y but to extract information using the predictors, for example, to identify subgroups. The main problem faced by unsupervised learning methods is the difficulty in validating the results given that there is no response variable available that allows them to be contrasted.\n",
    "\n",
    "The PCA method therefore allows \"condensing\" the information provided by multiple variables into just a few components. This makes it a very useful method to apply after using other statistical techniques such as regression, clustering... Even so, we must not forget that it is still necessary to have the value of the original variables to calculate the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619450f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570665bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA using Sklearn\n",
    "sns.pairplot(iris, hue=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba25ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73d9e4",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The result after doing fit_transform is that where we had 4 features we now have 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e94410",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "iris_t = pd.DataFrame(pca.fit_transform(iris.drop(columns=\"species\")), columns=[\"PC1\", \"PC2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aec40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF: PC1, PC2 & Species\n",
    "complete = iris_t.copy()\n",
    "complete[\"species\"] = iris[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86c89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c704c2e",
   "metadata": {},
   "source": [
    "## Deciding on the amount of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e27ba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing\n",
    "pca = PCA(n_components=None)\n",
    "pca.fit(iris.drop(columns=\"species\"))\n",
    "\n",
    "# Calculating variance\n",
    "exp_var = pca.explained_variance_ratio_ * 100\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "# Plotting\n",
    "plt.bar(range(1, 5), exp_var, align='center',label='Individual explained variance') #bars\n",
    "plt.step(range(1, 5), cum_exp_var, where='mid', label='Cumulative explained variance', color='red') #line\n",
    "\n",
    "# Axis\n",
    "plt.ylabel('Explained variance percentage')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.xticks(ticks=[1, 2, 3, 4])\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d22be4",
   "metadata": {},
   "source": [
    "pca.components has shape `[n_components, n_features]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f3e33",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## K-Means\n",
    "\n",
    "\n",
    "The oldest and the most popular. The idea is to previously select how many clusters are desired ($k$). The central points of the cluster (centroids) are selected randomly. For each new record, a cluster is reassigned and the centroid (mean) is recalculated. The process is iterated until there is no change in clustering.\n",
    "\n",
    "![kmeans](../images/kmeans.png)\n",
    "\n",
    "![kmeans2](../images/kmeans2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79853c3",
   "metadata": {},
   "source": [
    "Wait didn't we see KNN already?\n",
    "\n",
    "KNN is unsupervised for classification. K-means is unsupervised for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5dda24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_iris = sns.scatterplot(iris_t[\"PC1\"],iris_t[\"PC2\"], hue=iris[\"species\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07641d",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## We standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1a7d6",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Since KMeans is based on distance** and our variables have very different variances, even though they have the same unit, it would be a good idea to standardize the data if we had not done PCA.\n",
    "Anyway, I leave you an example of how the data would look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926eac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4182e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_ = iris.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a39138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_.drop(columns=\"species\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaler = pd.DataFrame(scaler.fit_transform(dat_), columns=dat_.columns)\n",
    "df_scaler.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ca24e",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Train the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b658e",
   "metadata": {},
   "source": [
    "### K-Means with the two PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0817605",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Since we are testing and understanding the algorithm, we are going to enter the number of clusters by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b11984",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3)\n",
    "km.fit(iris_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = km.predict(iris_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bdfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_t[\"predict\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e70cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_t[\"real\"] = iris[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727afa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_t.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(iris_t.predict, iris_t.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d81ed7",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### K-Means with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "km2  = KMeans(n_clusters=3)\n",
    "km2.fit(df_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc154a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_four = km2.predict(df_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaler[\"real\"] = iris.species\n",
    "df_scaler[\"kmeans\"] = y_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747561a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaler.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross2 = pd.crosstab(df_scaler.kmeans, df_scaler.real)\n",
    "cross2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867fc76",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Let's plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(20,7))\n",
    "pca_iris = sns.scatterplot(iris_t[\"PC1\"],iris_t[\"PC2\"], hue=iris_t[\"predict\"], ax=axs[0]);\n",
    "pca_iris = sns.scatterplot(iris_t[\"PC1\"],iris_t[\"PC2\"], hue=iris_t[\"real\"], ax=axs[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a5185",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## We measure the model / (Silhouette Score)\n",
    "silhouette punctuation\n",
    "The silhouette score is a metric of the separation of the clusters.\n",
    "\n",
    "It ranges from -1 to 1, where negative values ​​mean that the clusters are misallocated, 0 means that the clusters overlap, and 1 means that the clusters are separated and defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe5f17",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Score of the kmeans with the data WITH PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_t_onlypcs = iris_t.drop(columns=[\"predict\", \"real\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1301b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(iris_t_onlypcs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4823c39",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Score of the kmeans with the data WITHOUT PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b35f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaler_features = df_scaler.drop(columns=[\"real\", \"kmeans\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e27897",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(df_scaler_features, y_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a9ba5",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Elbow method\n",
    "Probably the best known method, where you calculate and plot the sum of squares at each number of clusters, and there you look for a change in slope from steep to shallow to determine the optimal number of clusters. This method is inaccurate, but still potentially useful.\n",
    "\n",
    "The elbow curve method is useful because it shows how increasing the number of clusters contributes to separating the clusters in a significant, not marginal, way. The curve indicates that additional groups beyond the third have little value. The elbow method is pretty neat, but a naive solution based on intracluster variance. The gap statistic is a more sophisticated method of dealing with data that has a distribution with no obvious clustering.\n",
    "\n",
    "This method works as follows: the sum of squared errors within the cluster is calculated for different values ​​of K and the K is chosen for which the sum of squared errors begins to decrease. This is visible as an elbow.\n",
    "\n",
    "Sum within a group of squared errors sounds a bit complex. Let's break it down:\n",
    "\n",
    "The squared error for each point is the square of the point's distance from its plot, that is, its predicted cluster center.\n",
    "The sum of squared errors score is the sum of these squared errors for all points.\n",
    "Any distance metric can be used, such as the Euclidean distance or the Manhattan distance.\n",
    "Now, to apply the elbow method and obtain the optimal number of clusters, it can be done as follows:\n",
    "\n",
    "Compute the clustering algorithm for different values ​​of K. For example, varying K from 1 to 10 clusters.\n",
    "For each K, compute the total sum of squares within the cluster.\n",
    "Plot the curve of the sum of squared errors according to the number of groups K.\n",
    "The location of a curve, elbow, on the graph is generally considered to be an indicator of the appropriate number of groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30404f1",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Inertia error method**: The more variety there is between the observations in the dataset, the greater their distances from their associated centroids. The inertia or intertia, in the context of K-Means, is the sum of all the distances of the observations of a cluster to its centroid. Assuming that the objective is to reduce the sum of distances (squares) of points with their respective centroids, the lower this total sum, the better, since it will indicate a greater homogeneity in the observations belonging to the created clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = [KMeans(n_clusters=i) for i in range(1,21)]\n",
    "for model in kmeans:\n",
    "    model.fit(dat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = [model.inertia_ for model in kmeans]\n",
    "inertias[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,21),inertias)\n",
    "plt.xticks(range(1,21));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69bc80",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We see that the exact cluster point we need to do for our data is 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

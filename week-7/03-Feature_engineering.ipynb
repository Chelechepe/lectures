{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da31d94a",
   "metadata": {},
   "source": [
    "# Steps to follow\n",
    "\n",
    "- Data\n",
    "- Process: clean, categorical variables\n",
    "- Data: X, y\n",
    "- Split\n",
    "- model.fit (train the model: up to you which one)\n",
    "- model.predict(X_test) -> in the future it'll a random X\n",
    "- Evaluate the model\n",
    "\n",
    "- Get rid off redundant info:\n",
    "    - RFE (scoring = \"r2\")\n",
    "    - Scores of the factors?\n",
    "- Split differently:cross validation K-folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dca77",
   "metadata": {},
   "source": [
    "##Â THINGS I can change\n",
    "- Dataset\n",
    "    - Get more info\n",
    "    - Get rid of info\n",
    "    - Change the one we have\n",
    "        - Standarization\n",
    "        - Normalization\n",
    "- Split\n",
    "    - Try different splits: cross-validation\n",
    "        \n",
    "- Models\n",
    "    - Choose different models\n",
    "    \n",
    "- Hyperparameters\n",
    "    - Choose different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-schema",
   "metadata": {},
   "source": [
    " # Hyperparameter Tuning - Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-christian",
   "metadata": {},
   "source": [
    "![nohayjupytersingif](https://media.giphy.com/media/jeDM590qtCP9C/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-stack",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#THINGS-I-can-change\" data-toc-modified-id=\"THINGS-I-can-change-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>THINGS I can change</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Small-exploration-of-the-data\" data-toc-modified-id=\"Small-exploration-of-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Small exploration of the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#We-look-at-the-&quot;Cabin&quot;-feature\" data-toc-modified-id=\"We-look-at-the-&quot;Cabin&quot;-feature-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>We look at the \"Cabin\" feature</a></span></li><li><span><a href=\"#We-analyze-the-names-of-the-passengers\" data-toc-modified-id=\"We-analyze-the-names-of-the-passengers-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>We analyze the names of the passengers</a></span></li><li><span><a href=\"#In-order-not-to-have-many-categories-with-the-titles,-we-are-going-to-keep-those-that-have-more-than-10\" data-toc-modified-id=\"In-order-not-to-have-many-categories-with-the-titles,-we-are-going-to-keep-those-that-have-more-than-10-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>In order not to have many categories with the titles, we are going to keep those that have more than 10</a></span></li></ul></li><li><span><a href=\"#Multicollinearity?\" data-toc-modified-id=\"Multicollinearity?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Multicollinearity?</a></span></li><li><span><a href=\"#Categorical-encoding\" data-toc-modified-id=\"Categorical-encoding-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Categorical encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Label-Encoder\" data-toc-modified-id=\"Label-Encoder-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Label Encoder</a></span></li><li><span><a href=\"#One-Hot-Encoder\" data-toc-modified-id=\"One-Hot-Encoder-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>One Hot Encoder</a></span></li><li><span><a href=\"#By-hand-with-a-dictionary-ðŸ’¡\" data-toc-modified-id=\"By-hand-with-a-dictionary-ðŸ’¡-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>By hand with a dictionary ðŸ’¡</a></span></li></ul></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standardization\" data-toc-modified-id=\"Standardization-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Standardization</a></span></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Normalization</a></span></li><li><span><a href=\"#I-quote-Andriy-Burkov:\" data-toc-modified-id=\"I-quote-Andriy-Burkov:-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>I quote Andriy Burkov:</a></span></li></ul></li><li><span><a href=\"#Summary-so-far\" data-toc-modified-id=\"Summary-so-far-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Summary so far</a></span></li><li><span><a href=\"#Let's-review:-Train-Test-Split\" data-toc-modified-id=\"Let's-review:-Train-Test-Split-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Let's review: Train-Test Split</a></span></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Random-sampling\" data-toc-modified-id=\"--Random-sampling-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>- Random sampling</a></span></li><li><span><a href=\"#--Grid-Sampling\" data-toc-modified-id=\"--Grid-Sampling-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>- Grid Sampling</a></span></li><li><span><a href=\"#--Bayesian-sampling\" data-toc-modified-id=\"--Bayesian-sampling-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>- Bayesian sampling</a></span></li><li><span><a href=\"#GridSearchCV-by-sklearn,-say-hello-to-your-new-friend!\" data-toc-modified-id=\"GridSearchCV-by-sklearn,-say-hello-to-your-new-friend!-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>GridSearchCV by sklearn, say hello to your new friend!</a></span></li><li><span><a href=\"#We-would-train-the-model-with-the-best-parameters\" data-toc-modified-id=\"We-would-train-the-model-with-the-best-parameters-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>We would train the model with the best parameters</a></span></li></ul></li><li><span><a href=\"#Save-/-Export-the-model\" data-toc-modified-id=\"Save-/-Export-the-model-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Save / Export the model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e4de2",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Feature engineering\n",
    "It is the process of using domain knowledge to extract features from raw data. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be thought of as applied machine learning itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e99115",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f59b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a941a",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Small exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef304ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d162b1c5",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We look at the \"Cabin\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dedec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c2b3096",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "There are many missing values, but we must use the stateroom variable because it can be an important predictor. As you can see in the picture below, first class had cabins on deck A, B, or C, a mix was on deck D or E, and third class was mostly on f or g. We can identify the cover by the first letter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-consent",
   "metadata": {},
   "source": [
    "![laimagendelbarco](../images/barco.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d79168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb788b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57595606",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We analyze the names of the passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4aa383",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The name could give us important information about the socioeconomic status of a passenger. And depending on their socioeconomic status, they have been able to buy a more expensive or cheaper ticket, which indicates a cabin located in one place or another on the ship. We can answer the question of whether or not someone is married or has a formal title and extract that information to generate a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-chosen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11e29484",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### In order not to have many categories with the titles, we are going to keep those that have more than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-colors",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a36315b1",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We only have the Age column with nulls... Let's fill them in, but exploring the data... are men the same age on average as women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-pottery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15d4cf22",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "To adjust a little more, we are going to fill the NaN of the age with the median but based on their gender and also based on the cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eba8ad",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87cdd971",
   "metadata": {},
   "source": [
    "## Multicollinearity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a30b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "456e1464",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Categorical encoding\n",
    "\n",
    "With an import line in our code, new possibilities are opened in options to how to model or manipulate our dataset.\n",
    "\n",
    "Previously we discussed in the dummies post the possibility of generating with the get_dummies() function and transforming each non-numeric data into a binary representation (expanding our dataset to the amount of different data that exists in a column)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed54885",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Label Encoder\n",
    "Pros and cons\n",
    "- If we have categories that have value in themselves, such as \"good, bad, regular\" the ideal would be not to let LabelEncoder do it automatically but to apply it manually, since the value we put can influence the weight that the algorithm gives those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6879cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-serial",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa7b29",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Now, as we have already mentioned, depending on the data we have, we could find situations in which, after encoding the labels, we could confuse our model by making it believe that a column has data with some kind of order or hierarchy, when clearly we don't have it. To avoid this, we \"OneHotEncode\" that column.\n",
    "What a hot encoding does is it takes a column that has categorical data, which has been encoded with labels, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has which value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-investigation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ce680b",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### By hand with a dictionary ðŸ’¡\n",
    "We can give a numerical value to each category and decide its importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-drain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-mattress",
   "metadata": {},
   "source": [
    "##Â Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a10db0",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Some algorithms, especially those based on distance calculations, will give more weight to features that show large changes in value, interpreting these features as artificially more important. For these algorithms, it is important that we scale our features, or that we scale features with naturally different scales, so that the features are used by the algorithm without artificial overweighting, and allow two features with different scales to be compared.\n",
    "\n",
    "Algorithms that do not require normalization/scaling are rule-based. They would not be affected by any monotonic transformation of the variables. Scaling is a monotonic transformation.\n",
    "Examples of algorithms in this category are all tree-based algorithms:\n",
    "- CART\n",
    "-Random Forests\n",
    "-Gradient Boosted Decision Trees\n",
    "These algorithms use rules (series of inequalities) and do not require normalization.\n",
    "\n",
    "There are two different types of feature scaling that we are going to explore:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cd15c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Still, I leave you literature:\n",
    "    - https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2118fec",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Standardization\n",
    "In standardization, we impose several statistical properties on the variable: the mean value is set to 0, and the standard deviation is set to 1. This is achieved by subtracting the mean from each feature value and dividing by the standard deviation. This is also sometimes called \"z-score normalization.\"\n",
    "\n",
    "So what does this mean, in practice, about standardized data? As we can see below, we now have the distributions of both variables centered around the zero mean, with a standard deviation of 1. Since we are enforcing this standard deviation, normalization reduces the effects of outliers on the feature. In addition, it allows comparing two characteristics with different scales or units. The different scales of the characteristics would be statistically reflected in differences in both the mean and the standard deviation. Standardizing these two numbers between features removes the influence of these scale differences.\n",
    "\n",
    "Standardization is especially important in situations where we use algorithms that assume features in our data are distributed along a 'bell curve' or Gaussian distribution, such as linear and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d505d2",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this dataset we do not face a regression problem but a classification problem,\n",
    "but let's do an example of standardization in a column to see the code\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-superintendent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5739ed12",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Normalization\n",
    "\n",
    "In the other form of feature scaling, called normalization, the feature is rescaled to a range between 0 and 1, without any change to its original distribution within that range. Mathematically, this is achieved by subtracting the minimum feature value from each feature value, and dividing by the difference between the largest value and the smallest value.\n",
    "\n",
    "Since we compute the normalized value using the maximum and minimum values â€‹â€‹of the feature, this technique is sometimes called \"min-max normalization.\"\n",
    "Normalization is most useful in cases where your data has few outliers but highly variable ranges, you don't know how your data is distributed, or you know that it is not distributed on a bell (Gaussian) curve. It is generally applied with algorithms that make no assumptions about the distributions of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-excuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b771a96",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### I quote Andriy Burkov:\n",
    "You may be wondering when normalization should be used and when standardization. There is no definitive answer to this question. Usually, if your dataset isn't too big and you have time, you can try both and see which one suits your task better.\n",
    "If you don't have time to run multiple experiments, as a general rule:\n",
    "\n",
    "- Unsupervised learning algorithms, in practice, benefit more from standardization than normalization.\n",
    "- Standardization is also preferable for a characteristic if the values â€‹â€‹it takes are distributed close to a normal distribution (the so-called bell curve).\n",
    "- Again, normalization is preferable for a feature if it can sometimes have extremely high or low values â€‹â€‹(outliers); this is because the normalization will \"squeeze\" the normal values â€‹â€‹into a very small range.\n",
    "- In all other cases, normalization is preferable.\n",
    "\n",
    "Feature rescaling is usually beneficial for most learning algorithms. However, modern implementations of learning algorithms, which can be found in popular libraries, are robust to features found in different ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560d306",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Summary so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504a776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784f876b",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Let's review: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565bf41",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "# Let's prepare the data (X, y) before training the model and tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f32eb3",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "# I assign the variables train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795ee82",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cb7fd",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "What is hyperparameter tuning?\n",
    "Hyperparameters are tunable parameters that allow you to control the process of training a model. For example, with neural networks, you can decide the number of hidden layers and the number of nodes in each layer. The performance of a model depends heavily on hyperparameters.\n",
    "Hyperparameter tuning, also called hyperparameter optimization, is the process of finding the hyperparameter configuration that produces the best performance. Typically, the process is manual and computationally expensive.\n",
    "\n",
    "There are different techniques to choose this hyperparameter tuning:\n",
    "    \n",
    "### - Random sampling\n",
    "Random sampling supports discrete and continuous hyperparameters. Supports early termination of underperforming strings. Some users perform an initial search with random sampling and then narrow the search space to improve results.\n",
    "In random sampling, hyperparameter values â€‹â€‹are randomly selected from the defined search space.\n",
    "\n",
    "### - Grid Sampling\n",
    "Grid sampling supports discrete hyperparameters. Use grid sampling if your budget allows you to search the search space exhaustively. Supports early termination of underperforming strings.\n",
    "\n",
    "### - Bayesian sampling\n",
    "Bayesian sampling is based on the Bayesian optimization algorithm. Pick the samples based on how the previous ones did, so that the new samples improve the main metric.\n",
    " For best results, it is recommended that the maximum number of runs be greater than or equal to 20 times the number of hyperparameters being optimized.\n",
    "The number of simultaneous series affects the efficiency of the adjustment process. Fewer concurrent runs can lead to better sampling convergence, since the lower degree of parallelism increases the number of runs that benefit from previously completed runs.\n",
    "\n",
    "We are going to look at grid hyperparameter tuning with GridSearchCV but I leave you to investigate Bayesian sampling with [HyperOpt](https://towardsdatascience.com/hyperopt-hyperparameter-tuning-based-on-bayesian-optimization-7fa32dffaf29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ec044",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### GridSearchCV by sklearn, say hello to your new friend!\n",
    "And read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0033302",
   "metadata": {},
   "source": [
    "![](https://najeesmith.github.io/images/Classifiers/RF/header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af6064",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "#RandomForest tunable hyperparameters\n",
    "parameters = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620823c",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "#Reduce to test with different n_estimators\n",
    "params = {\n",
    "     'n_estimators': [400, 600,800]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c856b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df4a15f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We would train the model with the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cef7c2",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "If GridSearchCV() is set to refit=True, after identifying the best hyperparameters, the model is retrained with them and stored in .best_estimator_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc36d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy\", round(accuracy_score(y_test,y_pred),3))\n",
    "print(\"Precission\",round(precision_score(y_test,y_pred, average = \"weighted\"),3))\n",
    "print(\"Recall\", round(recall_score(y_test,y_pred, average = \"weighted\"),3))\n",
    "print(\"F1_score\", round(f1_score(y_test,y_pred,average= \"weighted\"),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df9df5",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8977427",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Save / Export the model\n",
    "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211a51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
